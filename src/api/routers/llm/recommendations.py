"""
LLM Recommendations Router
LLM æ™ºèƒ½æ¨™ç±¤æ¨è–¦ç«¯é»
"""
from fastapi import APIRouter, HTTPException, Depends
import logging
import time
from typing import List, Dict

from models.requests import LLMRecommendRequest
from models.responses import (
    TagRecommendationResponse, 
    LLMTagRecommendation,
    QualityAssessment,
    RecommendationMetadata,
    PopularityTier
)
from services.supabase_client import get_supabase_service, SupabaseService
from services.keyword_expander import get_keyword_expander, KeywordExpander
from services.keyword_analyzer import get_keyword_analyzer, KeywordAnalyzer
from services import get_gpt5_nano_client, GPT5NanoClient, GPT5_AVAILABLE

logger = logging.getLogger(__name__)

router = APIRouter()


async def convert_gpt5_result_to_response(
    gpt5_result: Dict,
    db: SupabaseService,
    request: LLMRecommendRequest
) -> TagRecommendationResponse:
    """å°‡ GPT-5 Nano çµæœè½‰æ›ç‚ºæ¨™æº–å›æ‡‰æ ¼å¼"""
    try:
        logger.info(f"ğŸ”„ Converting GPT-5 result to response format")
        logger.info(f"GPT-5 result keys: {list(gpt5_result.keys())}")
        logger.info(f"GPT-5 result preview: {str(gpt5_result)[:500]}")
        
        # ç²å– GPT-5 æ¨è–¦çš„æ¨™ç±¤
        gpt5_tags = gpt5_result.get("tags", [])
        gpt5_categories = gpt5_result.get("categories", [])
        gpt5_confidence = gpt5_result.get("confidence", 0.8)
        gpt5_reasoning = gpt5_result.get("reasoning", "")
        gpt5_suggestions = gpt5_result.get("suggestions", [])
        
        logger.info(f"  - Tags count: {len(gpt5_tags)}")
        logger.info(f"  - Tags: {gpt5_tags}")
        logger.info(f"  - Confidence: {gpt5_confidence}")
        
        # å¾è³‡æ–™åº«æŸ¥è©¢æ¨™ç±¤è©³ç´°è³‡è¨Š
        recommended_tags = []
        for tag_name in gpt5_tags:
            tag_info = await db.get_tag_by_name(tag_name)
            if tag_info:
                # æ§‹å»º LLMTagRecommendation ç‰©ä»¶
                llm_tag = LLMTagRecommendation(
                    tag=tag_info['name'],
                    category=tag_info.get('main_category', tag_info.get('sub_category', 'UNKNOWN')),
                    subcategory=tag_info.get('sub_category'),
                    post_count=tag_info.get('post_count', 0),
                    confidence=gpt5_confidence,  # ä¿æŒ 0-1 ç¯„åœ
                    weight=int(gpt5_confidence * 10),  # è½‰æ›ç‚º 1-10 åˆ†æ•¸
                    popularity_tier=calculate_popularity_tier(tag_info.get('post_count', 0)),
                    match_reason=gpt5_reasoning,
                    usage_context=get_usage_context(tag_info.get('main_category', 'UNKNOWN')),
                    related_tags=[]  # æš«æ™‚ç‚ºç©ºï¼Œå¯ä»¥å¾ŒçºŒæ“´å±•
                )
                recommended_tags.append(llm_tag)
        
        # è¨ˆç®—åˆ†é¡åˆ†ä½ˆ
        category_distribution = {}
        for tag in recommended_tags:
            category = tag.category
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        # æ§‹å»ºå“è³ªè©•ä¼°
        # å°‡ 0-1 çš„ä¿¡å¿ƒåº¦è½‰æ›ç‚º 0-100 çš„åˆ†æ•¸
        overall_score = int(gpt5_confidence * 100)
        balance_score = min(len(category_distribution) * 25, 100)  # è¶Šå¤šåˆ†é¡è¶Šå¥½
        
        # è¨ˆç®—æµè¡Œåº¦åˆ†æ•¸ï¼ˆåŸºæ–¼æ¨™ç±¤çš„å¹³å‡ post_countï¼‰
        if recommended_tags:
            avg_post_count = sum(tag.post_count for tag in recommended_tags) / len(recommended_tags)
            popularity_score = min(int(avg_post_count / 10000 * 100), 100)
        else:
            popularity_score = 0
        
        quality_assessment = QualityAssessment(
            overall_score=overall_score,
            balance_score=balance_score,
            popularity_score=popularity_score,
            warnings=[]
        )
        
        # æ§‹å»ºå»ºè­°çš„ prompt
        suggested_prompt = ", ".join([tag.tag for tag in recommended_tags])
        
        # æ§‹å»ºå›æ‡‰
        response = TagRecommendationResponse(
            query=request.description,
            recommended_tags=recommended_tags,
            category_distribution=category_distribution,
            quality_assessment=quality_assessment,
            suggested_prompt=suggested_prompt,
            metadata=RecommendationMetadata(
                processing_time_ms=0,  # æœƒåœ¨å¤–éƒ¨è¨­ç½®
                total_candidates=len(gpt5_tags),
                algorithm="gpt-5-nano",
                cache_hit=False,
                keywords_extracted=gpt5_tags,
                keywords_expanded=gpt5_tags
            )
        )
        
        logger.info(f"GPT-5 Nano generated {len(recommended_tags)} tags")
        return response
        
    except Exception as e:
        logger.error(f"âŒ Failed to convert GPT-5 result: {e}", exc_info=True)
        logger.error(f"GPT-5 result that caused error: {gpt5_result}")
        raise HTTPException(status_code=500, detail=f"Failed to process GPT-5 result: {str(e)}")


def calculate_popularity_tier(post_count: int) -> PopularityTier:
    """è¨ˆç®—æµè¡Œåº¦ç­‰ç´š"""
    if post_count > 100000:
        return PopularityTier.VERY_POPULAR
    elif post_count > 10000:
        return PopularityTier.POPULAR
    elif post_count > 1000:
        return PopularityTier.MODERATE
    else:
        return PopularityTier.NICHE


def get_usage_context(category: str) -> str:
    """æ ¹æ“šåˆ†é¡è¿”å›ä½¿ç”¨æƒ…å¢ƒèªªæ˜"""
    context_map = {
        "CHARACTER": "è§’è‰²æ ¸å¿ƒæ¨™ç±¤ï¼Œå½±éŸ¿äººç‰©åŸºæœ¬ç‰¹å¾µå’Œæ•¸é‡",
        "CHARACTER_RELATED": "è§’è‰²ç›¸é—œæ¨™ç±¤ï¼Œæè¿°å¤–è§€ã€æœè£å’Œç´°ç¯€ç‰¹å¾µ",
        "ACTION_POSE": "å‹•ä½œå§¿æ…‹æ¨™ç±¤ï¼Œå½±éŸ¿äººç‰©å‹•ä½œã€è¡¨æƒ…å’Œå§¿å‹¢",
        "ENVIRONMENT": "ç’°å¢ƒæ¨™ç±¤ï¼Œè¨­å®šå ´æ™¯èƒŒæ™¯ã€åœ°é»å’Œæ°›åœ",
        "ART_STYLE": "è—è¡“é¢¨æ ¼æ¨™ç±¤ï¼Œå½±éŸ¿æ•´é«”ç•«é¢¨å’Œè¦–è¦ºå‘ˆç¾",
        "OBJECTS": "ç‰©ä»¶æ¨™ç±¤ï¼Œæ·»åŠ å ´æ™¯ä¸­çš„é“å…·å’Œç‰©å“",
        "COMPOSITION": "æ§‹åœ–æ¨™ç±¤ï¼Œå½±éŸ¿è¦–è§’ã€æ™¯æ·±å’Œç•«é¢æ§‹æˆ",
        "VISUAL_EFFECTS": "è¦–è¦ºæ•ˆæœæ¨™ç±¤ï¼Œæ·»åŠ å…‰å½±ã€ç‰¹æ•ˆå’Œæ°›åœ",
        "QUALITY": "å“è³ªæ¨™ç±¤ï¼Œæå‡æ•´é«”ç•«è³ªå’Œç²¾ç´°åº¦",
        "TECHNICAL": "æŠ€è¡“æ¨™ç±¤ï¼Œæ§åˆ¶ç”Ÿæˆåƒæ•¸å’Œé¢¨æ ¼ç´°ç¯€",
    }
    return context_map.get(category, "é€šç”¨æ¨™ç±¤ï¼Œå¯ç”¨æ–¼å„ç¨®å ´æ™¯")


def calculate_confidence(tag_name: str, keywords: List[str], expanded_keywords: List[str]) -> float:
    """
    è¨ˆç®—æ¨è–¦ä¿¡å¿ƒåº¦
    
    Args:
        tag_name: æ¨™ç±¤åç¨±
        keywords: åŸå§‹é—œéµå­—
        expanded_keywords: æ“´å±•å¾Œçš„é—œéµå­—
    """
    # å®Œå…¨åŒ¹é…
    if tag_name in keywords:
        return 0.98
    
    # æ“´å±•åŒ¹é…
    if tag_name in expanded_keywords:
        return 0.92
    
    # éƒ¨åˆ†åŒ¹é…
    for keyword in keywords:
        if keyword in tag_name or tag_name in keyword:
            return 0.85
    
    # åˆ†é¡åŒ¹é…
    return 0.70


@router.post(
    "/recommend-tags",
    response_model=TagRecommendationResponse,
    summary="ğŸ¤– æ™ºèƒ½æ¨™ç±¤æ¨è–¦",
    description="""
    **LLM å°ˆç”¨çš„ä¸€ç«™å¼æ¨™ç±¤æ¨è–¦ç«¯é»**
    
    è¼¸å…¥è‡ªç„¶èªè¨€æè¿°ï¼Œè¿”å›æœ€é©åˆçš„æ¨™ç±¤çµ„åˆï¼ŒåŒ…å«:
    - æ™ºèƒ½é—œéµå­—æå–å’Œæ“´å±•
    - åˆ†é¡å¹³è¡¡å„ªåŒ–
    - æµè¡Œåº¦åŠ æ¬Šæ’åº
    - è©³ç´°çš„ä½¿ç”¨å»ºè­°å’Œè§£é‡‹
    
    é€™æ˜¯ LLM æœ€å¸¸ç”¨çš„ç«¯é»ï¼Œä¸€æ¬¡èª¿ç”¨å®Œæˆæ‰€æœ‰å·¥ä½œã€‚
    """
)
async def recommend_tags(
    request: LLMRecommendRequest,
    db: SupabaseService = Depends(get_supabase_service),
    expander: KeywordExpander = Depends(get_keyword_expander),
    analyzer: KeywordAnalyzer = Depends(get_keyword_analyzer),
):
    """æ™ºèƒ½æ¨™ç±¤æ¨è–¦ï¼ˆå…©éšæ®µæœå°‹ï¼‰"""
    start_time = time.time()
    
    try:
        # GPT-5 Nano å„ªå…ˆé‚è¼¯ä¿æŒä¸è®Š
        if GPT5_AVAILABLE and get_gpt5_nano_client:
            gpt5_client = get_gpt5_nano_client()
            if gpt5_client.is_available():
                logger.info("Using GPT-5 Nano for tag recommendation")
                gpt5_result = await gpt5_client.generate_tags(request.description)
                if gpt5_result:
                    return await convert_gpt5_result_to_response(gpt5_result, db, request)
                else:
                    logger.warning("GPT-5 Nano failed, falling back to two-stage search")

        logger.info("Using two-stage search tag recommendation")
        
        # 1. é—œéµå­—æå–ã€æ“´å±•èˆ‡åˆ†æ
        original_keywords, expanded_keywords = expander.expand_query(request.description)
        if not original_keywords:
            raise HTTPException(status_code=400, detail="ç„¡æ³•å¾æè¿°ä¸­æå–ä»»ä½•é—œéµå­—")

        keyword_weights = analyzer.analyze_keyword_importance(original_keywords)
        primary_keyword = max(keyword_weights, key=keyword_weights.get)
        logger.info(f"Keywords: {original_keywords} -> Expanded: {len(expanded_keywords)}")
        logger.info(f"Primary keyword: '{primary_keyword}'")

        # 2. STAGE 1: ç²—ç¯© - ä½¿ç”¨ä¸»è¦é—œéµå­—ç²å–å¤§é‡å€™é¸æ¨™ç±¤
        # é€™è£¡æˆ‘å€‘ç›´æ¥å‘¼å« Supabase client çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ search_tags_by_keywords
        # å› ç‚º search_tags_by_keywords å…§éƒ¨å·²ç¶“åŒ…å«äº†æ’åºé‚è¼¯ï¼Œè€Œæˆ‘å€‘åªéœ€è¦åŸå§‹æ•¸æ“š
        query = db.client.table('tags_final').select('name, post_count, main_category, sub_category') \
                                             .ilike('name', f'%{primary_keyword}%') \
                                             .gte('post_count', request.min_popularity) \
                                             .limit(1000) # ç²å–å¤§é‡å€™é¸

        result = await query.execute()
        candidates = result.data

        if not candidates:
            logger.warning(f"No candidates found for primary keyword '{primary_keyword}'")
            # å¦‚æœä¸»è¦é—œéµå­—æ‰¾ä¸åˆ°ï¼Œå¯ä»¥è€ƒæ…®é™ç´šåˆ°ä½¿ç”¨æ‰€æœ‰é—œéµå­—æœå°‹
            candidates = await db.search_tags_by_keywords(
                keywords=expanded_keywords,
                limit=request.max_tags * 5,
                min_popularity=request.min_popularity,
                use_relevance_ranking=False # åœ¨æ­¤éšæ®µä¸éœ€æ’åº
            )
            if not candidates:
                raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°èˆ‡æ‚¨çš„æè¿°ç›¸é—œçš„æ¨™ç±¤")

        logger.info(f"Stage 1 (Coarse Filtering) found {len(candidates)} candidates.")

        # 3. STAGE 2: ç²¾æ’ - ä½¿ç”¨ relevance_scorer å°å€™é¸æ¨™ç±¤é€²è¡Œæ’åº
        # æˆ‘å€‘ç›´æ¥å‘¼å« relevance_scorer ä¸­çš„ rank_tags_by_relevance
        from services.relevance_scorer import rank_tags_by_relevance

        ranked_candidates = rank_tags_by_relevance(
            tags=candidates,
            keywords=original_keywords, # ä½¿ç”¨åŸå§‹é—œéµå­—é€²è¡Œç²¾ç¢ºæ’åº
            analyzer=analyzer,
            relevance_weight=0.7
        )
        
        # 4. åˆ†é¡å¹³è¡¡ (å¦‚æœå•Ÿç”¨) - é€™éƒ¨åˆ†é‚è¼¯å¯ä»¥è¤‡ç”¨
        if request.balance_categories:
            selected_tags = []
            categories_used = set()
            for tag in ranked_candidates:
                if len(selected_tags) >= request.max_tags:
                    break
                # æ·»åŠ åˆ°é¸æ“‡åˆ—è¡¨ï¼Œä¸¦è¨˜éŒ„å…¶åˆ†é¡
                selected_tags.append(tag)
                categories_used.add(tag.get('main_category'))
            
            # å¦‚æœåˆ†é¡ä¸å¤ å¤šæ¨£ï¼Œå˜—è©¦å¾å€™é¸è€…ä¸­è£œè¶³
            if len(categories_used) < 3 and len(ranked_candidates) > request.max_tags:
                for tag in ranked_candidates[request.max_tags:]:
                    if len(selected_tags) >= request.max_tags:
                        break
                    if tag.get('main_category') not in categories_used:
                        selected_tags.append(tag)
                        categories_used.add(tag.get('main_category'))

            final_tags = selected_tags
        else:
            final_tags = ranked_candidates[:request.max_tags]

        logger.info(f"Stage 2 (Fine Grained Ranking) selected {len(final_tags)} tags.")
        
        # 5. æ§‹å»ºæ¨è–¦åˆ—è¡¨
        recommendations = []
        for tag_data in final_tags:
            # 'relevance_score' ä¾†è‡ª rank_tags_by_relevance çš„å›å‚³
            confidence = tag_data.get('relevance_score', 0.7)

            recommendations.append(
                LLMTagRecommendation(
                    tag=tag_data['name'],
                    confidence=confidence,
                    popularity_tier=calculate_popularity_tier(tag_data.get('post_count', 0)),
                    post_count=tag_data.get('post_count', 0),
                    category=tag_data.get('main_category') or 'UNKNOWN',
                    subcategory=tag_data.get('sub_category'),
                    match_reason=f"åŒ¹é…ä¸»è¦é—œéµå­—: '{primary_keyword}'",
                    usage_context=get_usage_context(tag_data.get('main_category')),
                    weight=int(tag_data.get('final_score', 0.7) * 10),
                    related_tags=[]
                )
            )
        
        # 6. è¨ˆç®—åˆ†é¡åˆ†ä½ˆ
        category_distribution = {}
        for rec in recommendations:
            category_distribution[rec.category] = category_distribution.get(rec.category, 0) + 1
        
        # 7. å“è³ªè©•ä¼°
        overall_score = int(sum(r.confidence for r in recommendations) / len(recommendations) * 100) if recommendations else 0
        balance_score = min(len(category_distribution) * 25, 100)  # è¶Šå¤šåˆ†é¡è¶Šå¥½
        popularity_score = int(sum(r.post_count for r in recommendations) / len(recommendations) / 10000) if recommendations else 0
        popularity_score = min(popularity_score, 100)
        
        quality_assessment = QualityAssessment(
            overall_score=overall_score,
            balance_score=balance_score,
            popularity_score=popularity_score,
            warnings=[]
        )
        
        # æ·»åŠ è­¦å‘Š
        if len(recommendations) < request.max_tags:
            quality_assessment.warnings.append(
                f"åƒ…æ‰¾åˆ° {len(recommendations)} å€‹æ¨™ç±¤ï¼Œå°‘æ–¼è«‹æ±‚çš„ {request.max_tags} å€‹"
            )
        if balance_score < 50:
            quality_assessment.warnings.append("æ¨™ç±¤åˆ†é¡è¼ƒç‚ºå–®ä¸€ï¼Œå»ºè­°å¢åŠ ä¸åŒé¡å‹çš„é—œéµå­—")
        
        # 8. ç”Ÿæˆå»ºè­°çš„ prompt
        suggested_prompt = ", ".join([r.tag for r in recommendations])
        
        # 9. å…ƒè³‡æ–™
        processing_time = (time.time() - start_time) * 1000
        metadata = RecommendationMetadata(
            processing_time_ms=round(processing_time, 2),
            total_candidates=len(candidates),
            algorithm="keyword_matching_v1",
            cache_hit=False,
            keywords_extracted=original_keywords,
            keywords_expanded=expanded_keywords
        )
        
        return TagRecommendationResponse(
            query=request.description,
            recommended_tags=recommendations,
            category_distribution=category_distribution,
            quality_assessment=quality_assessment,
            suggested_prompt=suggested_prompt,
            metadata=metadata
        )
        
    except Exception as e:
        logger.error(f"Error in recommend_tags: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get(
    "/test-openai-config",
    summary="ğŸ”§ æ¸¬è©¦ OpenAI é…ç½®",
    description="""
    **æ¸¬è©¦ GPT-5 Nano é…ç½®å’Œé€£æ¥**
    
    æª¢æŸ¥ï¼š
    - API é‡‘é‘°æ˜¯å¦è¨­ç½®
    - ç’°å¢ƒè®Šæ•¸æ˜¯å¦æ­£ç¢º
    - æ¨¡å‹æ˜¯å¦å¯ç”¨
    - é€£æ¥æ¸¬è©¦
    """
)
async def test_openai_config():
    """æ¸¬è©¦ OpenAI é…ç½®"""
    gpt5_client = get_gpt5_nano_client()
    test_result = await gpt5_client.test_connection()
    
    return {
        "status": "success",
        "message": "OpenAI é…ç½®æ¸¬è©¦å®Œæˆ",
        "result": test_result
    }


async def _fallback_recommend_tags(
    request: LLMRecommendRequest,
    db: SupabaseService,
    expander: KeywordExpander,
    start_time: float
) -> TagRecommendationResponse:
    """
    GPT-5 ä¸å¯ç”¨æ™‚çš„é™ç´šæ–¹æ¡ˆ
    ä½¿ç”¨é—œéµå­—åŒ¹é…å’Œæ“´å±•ä¾†ç”Ÿæˆæ¨™ç±¤æ¨è–¦
    """
    try:
        logger.info("Using fallback keyword matching method")
        
        # 1. é—œéµå­—æå–å’Œæ“´å±•
        original_keywords = expander.extract_keywords(request.description)
        expanded_keywords = expander.expand_keywords(original_keywords)
        
        # 2. æœå°‹ç›¸é—œæ¨™ç±¤
        candidates = []
        for keyword in expanded_keywords:
            tags = await db.search_tags(keyword, limit=10)
            candidates.extend(tags)
        
        # 3. å»é‡å’Œæ’åº
        seen = set()
        unique_candidates = []
        for tag in candidates:
            if tag['name'] not in seen:
                seen.add(tag['name'])
                unique_candidates.append(tag)
        
        # æŒ‰ post_count æ’åº
        unique_candidates.sort(key=lambda x: x.get('post_count', 0), reverse=True)
        
        # 4. é™åˆ¶æ•¸é‡
        max_tags = request.max_tags or 20
        top_candidates = unique_candidates[:max_tags]
        
        # 5. æ§‹å»ºæ¨è–¦æ¨™ç±¤
        recommendations = []
        for tag in top_candidates:
            recommendation = LLMTagRecommendation(
                tag=tag['name'],
                confidence=0.7,  # é™ç´šæ–¹æ¡ˆçš„å›ºå®šä¿¡å¿ƒåº¦
                popularity_tier=calculate_popularity_tier(tag.get('post_count', 0)),
                post_count=tag.get('post_count', 0),
                category=tag.get('main_category', tag.get('sub_category', 'UNKNOWN')),
                subcategory=tag.get('sub_category'),
                match_reason="é—œéµå­—åŒ¹é…ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰",
                usage_context="åŸºæ–¼é—œéµå­—æœå°‹çš„æ¨è–¦",
                weight=7,
                related_tags=[]
            )
            recommendations.append(recommendation)
        
        # 6. è¨ˆç®—åˆ†é¡åˆ†ä½ˆ
        category_distribution = {}
        for rec in recommendations:
            category = rec.category
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        # 7. æ§‹å»ºå“è³ªè©•ä¼°
        quality_assessment = QualityAssessment(
            overall_score=0.7,
            warnings=["ä½¿ç”¨é™ç´šæ–¹æ¡ˆï¼šGPT-5 ä¸å¯ç”¨"],
            suggestions=["è«‹é…ç½® OpenAI API é‡‘é‘°ä»¥å•Ÿç”¨ AI æ¨è–¦åŠŸèƒ½"]
        )
        
        # 8. æ§‹å»ºå»ºè­°çš„ prompt
        suggested_prompt = ", ".join([rec.tag for rec in recommendations])
        
        # 9. å…ƒè³‡æ–™
        processing_time = (time.time() - start_time) * 1000
        metadata = RecommendationMetadata(
            processing_time_ms=round(processing_time, 2),
            total_candidates=len(candidates),
            algorithm="keyword_matching_fallback",
            cache_hit=False,
            keywords_extracted=original_keywords,
            keywords_expanded=expanded_keywords
        )
        
        logger.info(f"Fallback method generated {len(recommendations)} tags")
        
        return TagRecommendationResponse(
            query=request.description,
            recommended_tags=recommendations,
            category_distribution=category_distribution,
            quality_assessment=quality_assessment,
            suggested_prompt=suggested_prompt,
            metadata=metadata
        )
        
    except Exception as e:
        logger.error(f"Error in fallback method: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Fallback recommendation failed: {str(e)}")

