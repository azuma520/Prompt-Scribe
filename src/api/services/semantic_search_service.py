"""
èªç¾©æœå°‹æœå‹™

æä¾›åŸºæ–¼åµŒå…¥å‘é‡çš„èªç¾©æœå°‹åŠŸèƒ½ï¼Œæ•´åˆåˆ° Inspire Agent ä¸­ã€‚

Version: 2.0.0
Date: 2025-10-22
"""

import os
import json
import time
import logging
from typing import List, Dict, Optional
import numpy as np
from openai import AsyncOpenAI
from supabase import Client

from .inspire_db_wrapper import InspireDBWrapper
from ..models.inspire_models import SemanticSearchRequest, SemanticSearchResponse, SemanticSearchResult

logger = logging.getLogger(__name__)

class SemanticSearchService:
    """èªç¾©æœå°‹æœå‹™"""
    
    def __init__(self, db_wrapper: InspireDBWrapper, openai_client: AsyncOpenAI):
        self.db_wrapper = db_wrapper
        self.openai_client = openai_client
        self.supabase: Client = db_wrapper.client
    
    async def search(self, request: SemanticSearchRequest) -> SemanticSearchResponse:
        """
        åŸ·è¡Œèªç¾©æœå°‹
        
        Args:
            request: æœå°‹è«‹æ±‚
            
        Returns:
            SemanticSearchResponse: æœå°‹çµæœ
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ” Starting semantic search for query: '{request.query}'")
            
            # 1. ç”ŸæˆæŸ¥è©¢åµŒå…¥å‘é‡
            query_embedding = await self._generate_query_embedding(request.query)
            if not query_embedding:
                raise ValueError("Failed to generate query embedding")
            
            # 2. å„ªå…ˆé€é pgvector RPC åœ¨è³‡æ–™åº«ç«¯åšå…¨åº«ç›¸ä¼¼åº¦æ’åº
            results = await self._search_via_rpc(
                query_embedding=query_embedding,
                top_k=request.top_k,
                min_similarity=request.min_similarity,
                user_access_level=request.user_access_level
            )

            # è‹¥ RPC ä¸å¯ç”¨ï¼Œé€€å›æ‡‰ç”¨ç«¯æœ¬åœ°è¨ˆç®—ï¼ˆæœ€å¤šå– 1000 ç­†ï¼‰
            if not results:
                results = await self._search_similar_tags(
                    query_embedding=query_embedding,
                    top_k=request.top_k,
                    min_similarity=request.min_similarity,
                    user_access_level=request.user_access_level
                )
            
            # 3. ç²å–åµŒå…¥å‘é‡çµ±è¨ˆ
            embedding_count = await self._get_embedding_count()
            
            search_time_ms = (time.time() - start_time) * 1000
            
            logger.info(f"âœ… Semantic search completed in {search_time_ms:.1f}ms, found {len(results)} results")
            
            return SemanticSearchResponse(
                query=request.query,
                results=results,
                total_found=len(results),
                search_time_ms=search_time_ms,
                embedding_count=embedding_count
            )
            
        except Exception as e:
            logger.error(f"âŒ Semantic search failed: {e}")
            raise

    async def _search_via_rpc(
        self,
        query_embedding: List[float],
        top_k: int,
        min_similarity: float,
        user_access_level: str
    ) -> List[SemanticSearchResult]:
        """ä½¿ç”¨ Supabase RPCï¼ˆpgvectorï¼‰åœ¨è³‡æ–™åº«ç«¯åšç›¸ä¼¼åº¦æ’åºã€‚"""
        try:
            # èª¿ç”¨åœ¨ DB å…§å®šç¾©çš„ RPCï¼šsemantic_tag_search
            # æœŸæœ› DB ç«¯å·²å­˜åœ¨ï¼šembedding_vec vector(1536)ã€ivfflat ç´¢å¼•ã€RPC å‡½æ•¸
            payload = {
                "query_embedding": query_embedding,
                "match_count": top_k * 3,  # å¤šå–ä¸€é»å†åšæ¥­å‹™åŠ æ¬Š
                "min_similarity": max(min_similarity, 0.0),
            }

            # Supabase Python Client çš„ RPC èª¿ç”¨
            resp = self.supabase.rpc("semantic_tag_search", payload).execute()
            rows = resp.data or []
            if rows:
                logger.info("Using pgvector RPC for semantic search results")

            # æ‡‰ç”¨ç°¡å–®çš„æ¬Šé‡é‡æ’åºï¼ˆé¡åˆ¥ + ç†±åº¦ï¼‰
            scored: List[tuple[float, SemanticSearchResult]] = []
            for row in rows:
                similarity = float(row.get("similarity", 0.0))
                name = row.get("name") or row.get("tag_name")
                post_count = int(row.get("post_count", 0))
                main_category = row.get("main_category")
                sub_category = row.get("sub_category")

                if not self._is_content_allowed(name or "", user_access_level):
                    continue

                # é¡åˆ¥/ç†±åº¦å¾®é‡åŠ æ¬Š
                score = similarity
                if main_category:
                    score += 0.03
                if sub_category:
                    score += 0.02
                score += min(post_count, 1_000_000) / 1_000_000 * 0.02

                result = SemanticSearchResult(
                    name=name or "",
                    post_count=post_count,
                    similarity=similarity,
                    main_category=main_category,
                    sub_category=sub_category,
                )
                scored.append((score, result))

            # ä¾åŠ æ¬Šåˆ†æ•¸æ’åºï¼Œå–å‰ top_k
            if not scored:
                return []
            scored.sort(key=lambda x: x[0], reverse=True)
            return [r for _, r in scored[:top_k]]

        except Exception as e:
            # è‹¥ RPC ä¸å­˜åœ¨æˆ–éŒ¯èª¤ï¼Œè¿”å›ç©ºé™£åˆ—ä»¥è§¸ç™¼æœ¬åœ°é€€å›æ–¹æ¡ˆ
            logger.warning(f"âš ï¸ RPC semantic_tag_search not available or failed: {e}")
            return []
    
    async def _generate_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è©¢åµŒå…¥å‘é‡"""
        try:
            response = await self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=query
            )
            return response.data[0].embedding
            
        except Exception as e:
            logger.error(f"âŒ Failed to generate query embedding: {e}")
            return []
    
    async def _search_similar_tags(
        self, 
        query_embedding: List[float], 
        top_k: int,
        min_similarity: float,
        user_access_level: str
    ) -> List[SemanticSearchResult]:
        """æœå°‹ç›¸ä¼¼æ¨™ç±¤"""
        try:
            # ç²å–æ‰€æœ‰æœ‰åµŒå…¥å‘é‡çš„æ¨™ç±¤ï¼ˆé™åˆ¶æ•¸é‡ä»¥æé«˜æ€§èƒ½ï¼‰
            response = self.supabase.table('tags_final').select(
                'id, name, post_count, main_category, sub_category, embedding'
            ).not_.is_('embedding', 'null').limit(1000).execute()
            
            if not response.data:
                return []
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarities = []
            for tag in response.data:
                if tag['embedding']:
                    embedding_value = tag['embedding']
                    
                    # è™•ç†åµŒå…¥å‘é‡æ ¼å¼ï¼ˆJSON å­—ç¬¦ä¸²æˆ–ç›´æ¥æ•¸çµ„ï¼‰
                    if isinstance(embedding_value, str):
                        try:
                            embedding_value = json.loads(embedding_value)
                        except Exception:
                            continue
                    
                    similarity = self._cosine_similarity(query_embedding, embedding_value)
                    
                    # æ‡‰ç”¨ç›¸ä¼¼åº¦é–¾å€¼
                    if similarity >= min_similarity:
                        # æª¢æŸ¥å…§å®¹åˆ†ç´š
                        if self._is_content_allowed(tag['name'], user_access_level):
                            similarities.append(SemanticSearchResult(
                                name=tag['name'],
                                post_count=tag['post_count'],
                                similarity=similarity,
                                main_category=tag.get('main_category'),
                                sub_category=tag.get('sub_category')
                            ))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºä¸¦è¿”å›å‰ k å€‹
            similarities.sort(key=lambda x: x.similarity, reverse=True)
            return similarities[:top_k]
            
        except Exception as e:
            logger.error(f"âŒ Failed to search similar tags: {e}")
            return []
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        try:
            # å¼·åˆ¶è½‰æ›ç‚º float é¡å‹ä»¥é¿å… unicode æ•¸çµ„å•é¡Œ
            vec1 = np.asarray(vec1, dtype=float)
            vec2 = np.asarray(vec2, dtype=float)
            
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0
            
            return float(dot_product / (norm1 * norm2))
            
        except Exception as e:
            logger.error(f"âŒ Cosine similarity calculation failed: {e}")
            return 0
    
    def _is_content_allowed(self, tag_name: str, user_access_level: str) -> bool:
        """æª¢æŸ¥å…§å®¹æ˜¯å¦å…è¨±ï¼ˆåŸºæ–¼ä½¿ç”¨è€…æ¬Šé™ï¼‰"""
        # ç°¡å–®çš„å…§å®¹åˆ†ç´šæª¢æŸ¥
        nsfw_keywords = ['nude', 'sex', 'explicit', 'adult']
        
        if user_access_level == "all-ages":
            return not any(keyword in tag_name.lower() for keyword in nsfw_keywords)
        
        return True  # r15 å’Œ r18 å…è¨±æ‰€æœ‰å…§å®¹
    
    async def _get_embedding_count(self) -> int:
        """ç²å–å¯ç”¨åµŒå…¥å‘é‡æ•¸é‡"""
        try:
            response = self.supabase.table('tags_final').select(
                'id', count='exact'
            ).not_.is_('embedding', 'null').execute()
            
            return response.count or 0
            
        except Exception as e:
            logger.error(f"âŒ Failed to get embedding count: {e}")
            return 0

# ä¾è³´æ³¨å…¥å‡½æ•¸
def get_semantic_search_service(
    db_wrapper: InspireDBWrapper = None,
    openai_client: AsyncOpenAI = None
) -> SemanticSearchService:
    """ç²å–èªç¾©æœå°‹æœå‹™å¯¦ä¾‹"""
    if not db_wrapper:
        from services.inspire_db_wrapper import get_db_wrapper
        db_wrapper = get_db_wrapper()
    
    if not openai_client:
        from config import settings
        openai_client = AsyncOpenAI(api_key=settings.openai_api_key)
    
    return SemanticSearchService(db_wrapper, openai_client)
