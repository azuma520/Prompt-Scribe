# æ•¸æ“šé·ç§»æœ€ä½³å¯¦è¸èˆ‡è§£æ±ºæ–¹æ¡ˆ

## æ ¸å¿ƒæŒ‘æˆ°åˆ†æ

### é·ç§»æŒ‘æˆ°ç¸½çµ

#### 1. æ‰¹æ¬¡å¤§å°é™åˆ¶ï¼ˆToken Limitï¼‰
**å•é¡Œæœ¬è³ª**ï¼š
- MCP `read_file` å·¥å…·é™åˆ¶ï¼š**25,000 tokens/æ¬¡**
- æˆ‘å€‘çš„ mini-batch (500ç­†)ï¼šç´„ 18,000 tokens
- Tiny batch (1,500ç­†)ï¼šç´„ 27,173 tokensï¼ˆè¶…å‡ºé™åˆ¶ï¼‰

**æ ¹æœ¬åŸå› **ï¼š
- SQL INSERT èªå¥å†—é•·ï¼ˆæ¯è¡ŒåŒ…å«å®Œæ•´åˆ—åå’Œå€¼ï¼‰
- æ•¸æ“šåŒ…å«å¤§é‡æ–‡å­—å’Œç‰¹æ®Šå­—å…ƒ
- Token è¨ˆç®—åŒ…æ‹¬æ‰€æœ‰ SQL èªæ³•é–‹éŠ·

#### 2. COPY FROM STDIN ä¸æ”¯æ´
**æŠ€è¡“é™åˆ¶**ï¼š
```sql
-- PostgreSQL æœ€é«˜æ•ˆçš„æ‰¹é‡å°å…¥æ–¹å¼
COPY tags FROM STDIN;
-- MCP execute_sql ä¸æ”¯æ´æ•¸æ“šæµè¼¸å…¥
```

**å½±éŸ¿**ï¼š
- ç„¡æ³•ä½¿ç”¨æœ€é«˜æ•ˆçš„æ‰¹é‡å°å…¥æ–¹å¼
- å¿…é ˆä½¿ç”¨ INSERT èªå¥ï¼Œæ•ˆç‡é™ä½ 50-70%
- å¢åŠ ç¶²è·¯å‚³è¼¸é–‹éŠ·

#### 3. æª”æ¡ˆæ•¸é‡çˆ†ç‚¸
**å•é¡Œè¦æ¨¡**ï¼š
- ç¸½å…±ç”Ÿæˆï¼š**4,716 å€‹ SQL æª”æ¡ˆ**
- Mini batches: 282 å€‹
- Combined batches: 29 å€‹
- Small batches: 57 å€‹
- Tiny batches: 94 å€‹
- å…¶ä»–æ¸¬è©¦æª”æ¡ˆ: 4,254 å€‹

**ç®¡ç†æŒ‘æˆ°**ï¼š
- é›£ä»¥è¿½è¹¤ä¸Šå‚³é€²åº¦
- æ‰¹æ¬¡åŸ·è¡Œé †åºæ§åˆ¶
- éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

---

## æœ€ä½³å¯¦è¸è§£æ±ºæ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: Supabase CLI + PostgreSQL COPYï¼ˆæ¨è–¦ï¼‰â­â­â­

#### å„ªå‹¢
- âœ… **æ•ˆç‡æœ€é«˜**ï¼š5-10 åˆ†é˜å®Œæˆå…¨éƒ¨é·ç§»
- âœ… **åŸç”Ÿæ”¯æ´**ï¼šPostgreSQL COPY å‘½ä»¤
- âœ… **å¯é æ€§é«˜**ï¼šå°ˆæ¥­çš„é·ç§»å·¥å…·
- âœ… **ç°¡å–®æ˜“ç”¨**ï¼š3 æ¢å‘½ä»¤å®Œæˆ

#### å¯¦æ–½æ­¥é©Ÿ

**1. å®‰è£ Supabase CLI**
```bash
# Windows (Scoop)
scoop bucket add supabase https://github.com/supabase/scoop-bucket.git
scoop install supabase

# macOS/Linux (Homebrew)
brew install supabase/tap/supabase

# æˆ–ä½¿ç”¨ npm
npm install -g supabase
```

**2. ç™»å…¥ä¸¦é€£çµå°ˆæ¡ˆ**
```bash
# ç™»å…¥ Supabase
supabase login

# é€£çµåˆ°ä½ çš„å°ˆæ¡ˆ
supabase link --project-ref fumuvmbhmmzkenizksyq
```

**3. æ–¹å¼ Aï¼šä½¿ç”¨ PostgreSQL COPYï¼ˆæœ€å¿«ï¼‰**
```bash
# ä½¿ç”¨ç’°å¢ƒè®Šæ•¸è¨­å®šé€£æ¥
export PGHOST="db.fumuvmbhmmzkenizksyq.supabase.co"
export PGPORT="5432"
export PGUSER="postgres"
export PGPASS="your-password"
export PGDATABASE="postgres"

# ä½¿ç”¨ psql COPY å‘½ä»¤
psql -c "\COPY tags(id, name, tag_type, category, post_count, created_at) FROM 'stage1/output/tags_data.csv' WITH (FORMAT csv, HEADER true)"
```

**4. æ–¹å¼ Bï¼šä½¿ç”¨ Supabase CLI åŸ·è¡Œ SQL**
```bash
# å‰µå»ºå®Œæ•´çš„ SQL æª”æ¡ˆ
cat stage1/output/mini_batches/*.sql > complete_migration.sql

# åŸ·è¡Œé·ç§»
supabase db execute -f complete_migration.sql
```

**5. æ–¹å¼ Cï¼šä½¿ç”¨ ogr2ogrï¼ˆé‡å° GeoJSON/å¤§å‹æ•¸æ“šï¼‰**
```bash
# å¦‚æœæœ‰åœ°ç†æ•¸æ“šæˆ–è¶…å¤§å‹ JSON
PG_USE_COPY=true ogr2ogr -f pgdump output.sql input.json
supabase db execute -f output.sql
```

#### æ€§èƒ½å„ªåŒ–æŠ€å·§

**å„ªåŒ– 1ï¼šèª¿æ•´ PostgreSQL åƒæ•¸**
```sql
-- æš«æ™‚æé«˜æ‰¹æ¬¡å¤§å°ï¼ˆé‡å° FDWï¼‰
ALTER SERVER supabase_server OPTIONS (batch_size '10000');

-- å„ªåŒ–å¯«å…¥æ€§èƒ½
SET wal_buffers = '64MB';
SET max_wal_senders = 0;
SET statement_timeout = 0;
SET work_mem = '2GB';
```

**å„ªåŒ– 2ï¼šæš«æ™‚ç¦ç”¨è§¸ç™¼å™¨**
```sql
-- é·ç§»å‰ç¦ç”¨è§¸ç™¼å™¨
ALTER TABLE tags DISABLE TRIGGER ALL;

-- åŸ·è¡Œé·ç§»
\COPY tags FROM 'tags_data.csv' WITH CSV HEADER;

-- é·ç§»å¾Œé‡æ–°å•Ÿç”¨
ALTER TABLE tags ENABLE TRIGGER ALL;
```

**å„ªåŒ– 3ï¼šä½¿ç”¨ UNLOGGED è¡¨ï¼ˆè‡¨æ™‚ï¼‰**
```sql
-- å‰µå»ºè‡¨æ™‚ UNLOGGED è¡¨ï¼ˆæ›´å¿«ï¼‰
CREATE UNLOGGED TABLE tags_temp (LIKE tags INCLUDING ALL);

-- å°å…¥æ•¸æ“šåˆ°è‡¨æ™‚è¡¨
\COPY tags_temp FROM 'tags_data.csv' WITH CSV HEADER;

-- æ’åºä¸¦æ’å…¥åˆ°æ­£å¼è¡¨
INSERT INTO tags SELECT * FROM tags_temp ORDER BY id;

-- åˆªé™¤è‡¨æ™‚è¡¨
DROP TABLE tags_temp;
```

---

### æ–¹æ¡ˆ 2: Supabase REST API æ‰¹é‡ä¸Šå‚³

#### å„ªå‹¢
- âœ… **ç¨‹å¼åŒ–æ§åˆ¶**ï¼šå®Œæ•´çš„éŒ¯èª¤è™•ç†
- âœ… **æ•´åˆå‹å¥½**ï¼šPython/JavaScript åŸç”Ÿæ”¯æ´
- âœ… **æ¼¸é€²å¼é·ç§»**ï¼šæ”¯æ´æ–·é»çºŒå‚³

#### å¯¦æ–½ä»£ç¢¼

**Python å¯¦ç¾**
```python
import requests
import pandas as pd
import time
from tqdm import tqdm

# é…ç½®
SUPABASE_URL = "https://fumuvmbhmmzkenizksyq.supabase.co"
SUPABASE_KEY = "your-service-role-key"  # ä½¿ç”¨ Service Role Key

# REST API endpoint
url = f"{SUPABASE_URL}/rest/v1/tags"
headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=minimal"  # æ¸›å°‘å›å‚³æ•¸æ“š
}

# è®€å–æ•¸æ“š
df = pd.read_csv('stage1/output/tags_data.csv')
total_rows = len(df)

# æ‰¹æ¬¡ä¸Šå‚³è¨­å®š
BATCH_SIZE = 1000  # æ¯æ‰¹ 1000 ç­†
uploaded = 0
failed_batches = []

# æ–·é»çºŒå‚³æ”¯æ´
checkpoint_file = 'upload_checkpoint.txt'

# è®€å–ä¸Šæ¬¡é€²åº¦
try:
    with open(checkpoint_file, 'r') as f:
        uploaded = int(f.read().strip())
    print(f"å¾æª¢æŸ¥é»ç¹¼çºŒ: {uploaded}/{total_rows}")
except FileNotFoundError:
    uploaded = 0

# æ‰¹æ¬¡ä¸Šå‚³
for i in tqdm(range(uploaded, total_rows, BATCH_SIZE)):
    batch = df[i:i+BATCH_SIZE].to_dict('records')
    
    try:
        response = requests.post(url, json=batch, headers=headers, timeout=30)
        
        if response.status_code == 201:
            uploaded = i + len(batch)
            # æ›´æ–°æª¢æŸ¥é»
            with open(checkpoint_file, 'w') as f:
                f.write(str(uploaded))
        else:
            print(f"æ‰¹æ¬¡ {i//BATCH_SIZE + 1} å¤±æ•—: {response.status_code}")
            failed_batches.append(i)
            
    except Exception as e:
        print(f"æ‰¹æ¬¡ {i//BATCH_SIZE + 1} éŒ¯èª¤: {e}")
        failed_batches.append(i)
        time.sleep(5)  # éŒ¯èª¤å¾Œç­‰å¾…

print(f"å®Œæˆ! ä¸Šå‚³: {uploaded}/{total_rows}")
if failed_batches:
    print(f"å¤±æ•—æ‰¹æ¬¡: {failed_batches}")
```

**JavaScript/TypeScript å¯¦ç¾**
```typescript
import { createClient } from '@supabase/supabase-js'
import fs from 'fs'
import csv from 'csv-parser'

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
)

async function bulkInsert() {
  const BATCH_SIZE = 1000
  let batch: any[] = []
  let totalInserted = 0

  const stream = fs.createReadStream('tags_data.csv')
    .pipe(csv())

  for await (const row of stream) {
    batch.push(row)

    if (batch.length >= BATCH_SIZE) {
      const { error } = await supabase
        .from('tags')
        .insert(batch)
      
      if (error) {
        console.error('Batch insert failed:', error)
      } else {
        totalInserted += batch.length
        console.log(`Inserted: ${totalInserted}`)
      }
      
      batch = []
    }
  }

  // æ’å…¥å‰©é¤˜æ•¸æ“š
  if (batch.length > 0) {
    await supabase.from('tags').insert(batch)
    totalInserted += batch.length
  }

  console.log(`Total inserted: ${totalInserted}`)
}

bulkInsert()
```

---

### æ–¹æ¡ˆ 3: MCP è‡ªå‹•åŒ–å¾®æ‰¹æ¬¡ï¼ˆç¾æœ‰å·¥å…·ï¼‰

#### å„ªå‹¢
- âœ… **ç„¡éœ€é¡å¤–å®‰è£**ï¼šä½¿ç”¨ç¾æœ‰ MCP å·¥å…·
- âœ… **å®‰å…¨æ€§é«˜**ï¼šMCP æä¾›çš„æŠ½è±¡å±¤
- âœ… **å·²é©—è­‰**ï¼šæ¸¬è©¦æˆåŠŸä¸Šå‚³ 500 ç­†

#### æ”¹é€²ç­–ç•¥

**ç­–ç•¥ 1ï¼šåˆ†å¡Šè®€å– SQL æª”æ¡ˆ**
```python
import os
from pathlib import Path

def read_sql_in_chunks(file_path, max_tokens=20000):
    """
    å°‡å¤§å‹ SQL æª”æ¡ˆåˆ†å¡Šè®€å–ï¼Œç¢ºä¿æ¯å¡Šä¸è¶…é token é™åˆ¶
    """
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # ç²—ç•¥ä¼°ç®— tokensï¼ˆ1 token â‰ˆ 4 å­—å…ƒï¼‰
            line_tokens = len(line) // 4
            
            if current_tokens + line_tokens > max_tokens:
                chunks.append(''.join(current_chunk))
                current_chunk = [line]
                current_tokens = line_tokens
            else:
                current_chunk.append(line)
                current_tokens += line_tokens
    
    if current_chunk:
        chunks.append(''.join(current_chunk))
    
    return chunks

# ä½¿ç”¨ç¯„ä¾‹
sql_file = "stage1/output/mini_batches/batch_0001.sql"
chunks = read_sql_in_chunks(sql_file, max_tokens=20000)

for i, chunk in enumerate(chunks):
    print(f"åŸ·è¡Œ chunk {i+1}/{len(chunks)}")
    # ä½¿ç”¨ MCP execute_sql åŸ·è¡Œ
    # execute_sql(chunk)
```

**ç­–ç•¥ 2ï¼šå‰µå»ºé€²åº¦è¿½è¹¤ç³»çµ±**
```python
import json
from datetime import datetime

class UploadProgressTracker:
    def __init__(self, checkpoint_file='upload_progress.json'):
        self.checkpoint_file = checkpoint_file
        self.progress = self.load_progress()
    
    def load_progress(self):
        try:
            with open(self.checkpoint_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                'total_batches': 282,
                'uploaded_batches': [],
                'failed_batches': [],
                'last_update': None
            }
    
    def save_progress(self):
        self.progress['last_update'] = datetime.now().isoformat()
        with open(self.checkpoint_file, 'w') as f:
            json.dump(self.progress, f, indent=2)
    
    def mark_uploaded(self, batch_id):
        if batch_id not in self.progress['uploaded_batches']:
            self.progress['uploaded_batches'].append(batch_id)
            self.save_progress()
    
    def mark_failed(self, batch_id, error):
        self.progress['failed_batches'].append({
            'batch_id': batch_id,
            'error': str(error),
            'timestamp': datetime.now().isoformat()
        })
        self.save_progress()
    
    def get_pending_batches(self):
        all_batches = set(range(1, self.progress['total_batches'] + 1))
        uploaded = set(self.progress['uploaded_batches'])
        return sorted(all_batches - uploaded)

# ä½¿ç”¨ç¯„ä¾‹
tracker = UploadProgressTracker()
pending = tracker.get_pending_batches()
print(f"å¾…ä¸Šå‚³æ‰¹æ¬¡: {len(pending)}")
```

**ç­–ç•¥ 3ï¼šæ™ºèƒ½é‡è©¦æ©Ÿåˆ¶**
```python
import time
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    
                    delay = base_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    print(f"é‡è©¦ {attempt + 1}/{max_retries}ï¼Œç­‰å¾… {delay}s...")
                    time.sleep(delay)
        return wrapper
    return decorator

@retry_with_backoff(max_retries=3, base_delay=2)
def upload_batch_with_mcp(batch_sql):
    # ä½¿ç”¨ MCP execute_sql
    # result = mcp_execute_sql(batch_sql)
    pass
```

---

## æŠ€è¡“é™åˆ¶æ‡‰å°ç­–ç•¥

### é™åˆ¶ 1: MCP Token é™åˆ¶ (25K)

#### è§£æ±ºæ–¹æ¡ˆ Aï¼šå‹•æ…‹æ‰¹æ¬¡å¤§å°èª¿æ•´
```python
def calculate_optimal_batch_size(sample_records, target_tokens=20000):
    """
    æ ¹æ“šæ¨£æœ¬æ•¸æ“šå‹•æ…‹è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
    """
    # å–æ¨£æœ¬è¨ˆç®—å¹³å‡ token æ•¸
    sample_sql = generate_insert_sql(sample_records[:10])
    avg_tokens_per_record = len(sample_sql) // (4 * 10)  # ç²—ç•¥ä¼°ç®—
    
    # è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°ï¼ˆç•™ 20% ç·©è¡ï¼‰
    optimal_size = int((target_tokens * 0.8) / avg_tokens_per_record)
    
    return max(100, min(optimal_size, 1000))  # é™åˆ¶åœ¨ 100-1000 ä¹‹é–“
```

#### è§£æ±ºæ–¹æ¡ˆ Bï¼šå£“ç¸® SQL èªæ³•
```python
def generate_compact_insert_sql(records, table_name='tags'):
    """
    ç”Ÿæˆç·Šæ¹Šçš„ INSERT èªå¥ï¼Œæ¸›å°‘ token ä½¿ç”¨
    """
    if not records:
        return ""
    
    # åªåœ¨ç¬¬ä¸€è¡Œå¯«åˆ—å
    columns = list(records[0].keys())
    column_str = ','.join(columns)
    
    # æ‰¹é‡å€¼
    values = []
    for record in records:
        value_list = [f"'{v}'" if isinstance(v, str) else str(v) 
                     for v in record.values()]
        values.append(f"({','.join(value_list)})")
    
    # å–®ä¸€ INSERT èªå¥ï¼ˆæ›´ç·Šæ¹Šï¼‰
    sql = f"INSERT INTO {table_name}({column_str}) VALUES\n"
    sql += ",\n".join(values) + ";"
    
    return sql
```

### é™åˆ¶ 2: COPY FROM STDIN ä¸æ”¯æ´

#### è§£æ±ºæ–¹æ¡ˆï¼šä½¿ç”¨ postgres_fdw + COPY
```sql
-- æ­¥é©Ÿ 1ï¼šå‰µå»ºå¤–éƒ¨è¡¨é€£æ¥
CREATE EXTENSION IF NOT EXISTS postgres_fdw;

CREATE SERVER remote_server
  FOREIGN DATA WRAPPER postgres_fdw
  OPTIONS (
    host 'db.fumuvmbhmmzkenizksyq.supabase.co',
    port '5432',
    dbname 'postgres'
  );

CREATE USER MAPPING FOR current_user
  SERVER remote_server
  OPTIONS (
    user 'postgres',
    password 'your-password'
  );

-- æ­¥é©Ÿ 2ï¼šå‰µå»ºå¤–éƒ¨è¡¨
CREATE FOREIGN TABLE tags_remote (
  id BIGINT,
  name TEXT,
  tag_type TEXT,
  category TEXT,
  post_count INTEGER,
  created_at TIMESTAMP
)
SERVER remote_server
OPTIONS (
  schema_name 'public',
  table_name 'tags'
);

-- æ­¥é©Ÿ 3ï¼šå„ªåŒ–æ‰¹æ¬¡å¤§å°
ALTER SERVER remote_server OPTIONS (batch_size '10000');

-- æ­¥é©Ÿ 4ï¼šæ‰¹é‡æ’å…¥
INSERT INTO tags_remote
SELECT * FROM local_tags;
```

---

## å®Œæ•´è‡ªå‹•åŒ–æ–¹æ¡ˆ

### ç«¯åˆ°ç«¯è‡ªå‹•åŒ–è…³æœ¬

```python
#!/usr/bin/env python3
"""
Supabase æ•¸æ“šé·ç§»è‡ªå‹•åŒ–å·¥å…·
æ”¯æ´å¤šç¨®é·ç§»æ–¹å¼å’Œå®Œæ•´çš„éŒ¯èª¤è™•ç†
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Optional
import pandas as pd
import requests
from tqdm import tqdm

class SupabaseMigrator:
    def __init__(self, 
                 supabase_url: str, 
                 supabase_key: str,
                 method: str = 'rest_api'):
        self.supabase_url = supabase_url
        self.supabase_key = supabase_key
        self.method = method
        self.progress_file = 'migration_progress.json'
    
    def migrate(self, 
                csv_file: str, 
                table_name: str = 'tags',
                batch_size: int = 1000):
        """åŸ·è¡Œé·ç§»"""
        
        if self.method == 'rest_api':
            return self._migrate_via_rest_api(csv_file, table_name, batch_size)
        elif self.method == 'cli':
            return self._migrate_via_cli(csv_file, table_name)
        elif self.method == 'mcp':
            return self._migrate_via_mcp(csv_file, table_name, batch_size)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ–¹æ³•: {self.method}")
    
    def _migrate_via_rest_api(self, csv_file, table_name, batch_size):
        """REST API æ–¹å¼é·ç§»"""
        print(f"ä½¿ç”¨ REST API é·ç§»æ•¸æ“š...")
        
        # è®€å–æ•¸æ“š
        df = pd.read_csv(csv_file)
        total_rows = len(df)
        
        # REST API endpoint
        url = f"{self.supabase_url}/rest/v1/{table_name}"
        headers = {
            "apikey": self.supabase_key,
            "Authorization": f"Bearer {self.supabase_key}",
            "Content-Type": "application/json",
            "Prefer": "return=minimal"
        }
        
        # è¼‰å…¥é€²åº¦
        progress = self._load_progress()
        uploaded = progress.get('uploaded_rows', 0)
        
        # æ‰¹æ¬¡ä¸Šå‚³
        for i in tqdm(range(uploaded, total_rows, batch_size)):
            batch = df[i:i+batch_size].to_dict('records')
            
            try:
                response = requests.post(
                    url, 
                    json=batch, 
                    headers=headers, 
                    timeout=60
                )
                response.raise_for_status()
                
                uploaded = i + len(batch)
                self._save_progress({'uploaded_rows': uploaded})
                
            except Exception as e:
                print(f"\néŒ¯èª¤: {e}")
                return False
        
        print(f"\nå®Œæˆ! ç¸½å…±ä¸Šå‚³ {total_rows} ç­†æ•¸æ“š")
        return True
    
    def _migrate_via_cli(self, csv_file, table_name):
        """CLI æ–¹å¼é·ç§»ï¼ˆæœ€å¿«ï¼‰"""
        print(f"ä½¿ç”¨ Supabase CLI é·ç§»æ•¸æ“š...")
        
        # ç”Ÿæˆ COPY å‘½ä»¤
        copy_cmd = f"""
        \\COPY {table_name} FROM '{csv_file}' WITH (
            FORMAT csv,
            HEADER true,
            DELIMITER ',',
            ENCODING 'UTF8'
        );
        """
        
        # åŸ·è¡Œå‘½ä»¤
        import subprocess
        result = subprocess.run(
            ['supabase', 'db', 'execute', '-c', copy_cmd],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print("é·ç§»æˆåŠŸ!")
            return True
        else:
            print(f"éŒ¯èª¤: {result.stderr}")
            return False
    
    def _load_progress(self) -> Dict:
        """è¼‰å…¥é€²åº¦"""
        try:
            with open(self.progress_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}
    
    def _save_progress(self, progress: Dict):
        """ä¿å­˜é€²åº¦"""
        with open(self.progress_file, 'w') as f:
            json.dump(progress, f, indent=2)

def main():
    parser = argparse.ArgumentParser(description='Supabase æ•¸æ“šé·ç§»å·¥å…·')
    parser.add_argument('--csv', required=True, help='CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--table', default='tags', help='ç›®æ¨™è¡¨å')
    parser.add_argument('--method', 
                       choices=['rest_api', 'cli', 'mcp'],
                       default='rest_api',
                       help='é·ç§»æ–¹å¼')
    parser.add_argument('--batch-size', type=int, default=1000, 
                       help='æ‰¹æ¬¡å¤§å°')
    
    args = parser.parse_args()
    
    # å¾ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
    supabase_url = os.getenv('SUPABASE_URL')
    supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
    
    if not supabase_url or not supabase_key:
        print("éŒ¯èª¤: è«‹è¨­å®š SUPABASE_URL å’Œ SUPABASE_SERVICE_ROLE_KEY ç’°å¢ƒè®Šæ•¸")
        sys.exit(1)
    
    # åŸ·è¡Œé·ç§»
    migrator = SupabaseMigrator(supabase_url, supabase_key, args.method)
    success = migrator.migrate(args.csv, args.table, args.batch_size)
    
    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()
```

---

## æ¨è–¦å¯¦æ–½è·¯å¾‘

### ğŸ¯ æœ€ä½³å¯¦è¸æµç¨‹

1. **æº–å‚™éšæ®µ**ï¼ˆ5 åˆ†é˜ï¼‰
   ```bash
   # å®‰è£ Supabase CLI
   npm install -g supabase
   
   # ç™»å…¥ä¸¦é€£çµå°ˆæ¡ˆ
   supabase login
   supabase link --project-ref fumuvmbhmmzkenizksyq
   ```

2. **é·ç§»åŸ·è¡Œ**ï¼ˆ5-10 åˆ†é˜ï¼‰
   ```bash
   # æ–¹å¼ 1ï¼šç›´æ¥ä½¿ç”¨ COPYï¼ˆæœ€å¿«ï¼‰
   supabase db execute -c "\COPY tags FROM 'stage1/output/tags_data.csv' WITH CSV HEADER"
   
   # æ–¹å¼ 2ï¼šä½¿ç”¨è‡ªå‹•åŒ–è…³æœ¬
   python migrate_to_supabase.py \
     --csv stage1/output/tags_data.csv \
     --method cli
   ```

3. **é©—è­‰éšæ®µ**ï¼ˆ2 åˆ†é˜ï¼‰
   ```sql
   -- æª¢æŸ¥è¨˜éŒ„æ•¸
   SELECT COUNT(*) FROM tags;
   -- é æœŸ: 140,782
   
   -- æª¢æŸ¥æ•¸æ“šç¯„åœ
   SELECT MIN(id), MAX(id) FROM tags;
   
   -- æŠ½æ¨£é©—è­‰
   SELECT * FROM tags LIMIT 10;
   ```

4. **å„ªåŒ–éšæ®µ**ï¼ˆå¯é¸ï¼‰
   ```sql
   -- å‰µå»ºç´¢å¼•
   CREATE INDEX idx_tags_name ON tags(name);
   CREATE INDEX idx_tags_category ON tags(category);
   
   -- åˆ†æè¡¨çµ±è¨ˆ
   ANALYZE tags;
   ```

---

## ç¸½çµ

### âœ… æ¨è–¦æ–¹æ¡ˆæ’åº

1. **Supabase CLI + COPY** - æœ€å¿«ã€æœ€å¯é 
2. **REST API æ‰¹é‡ä¸Šå‚³** - éˆæ´»ã€å¯æ§
3. **MCP å¾®æ‰¹æ¬¡** - ç„¡éœ€å®‰è£ã€å®‰å…¨

### ğŸ“Š æ€§èƒ½å°æ¯”

| æ–¹æ¡ˆ | é ä¼°æ™‚é–“ | å¯é æ€§ | è¤‡é›œåº¦ | æ¨è–¦åº¦ |
|------|---------|--------|--------|--------|
| CLI + COPY | 5-10 åˆ†é˜ | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ |
| REST API | 30-60 åˆ†é˜ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| MCP å¾®æ‰¹æ¬¡ | 1-2 å°æ™‚ | â­â­â­ | â­â­â­â­ | â­â­â­ |

### ğŸš€ ç«‹å³è¡Œå‹•

é¸æ“‡ **Supabase CLI** æ–¹æ¡ˆï¼Œ3 æ¢å‘½ä»¤å®Œæˆé·ç§»ï¼š
```bash
supabase login
supabase link --project-ref fumuvmbhmmzkenizksyq
supabase db execute -c "\COPY tags FROM 'stage1/output/tags_data.csv' WITH CSV HEADER"
```

---

*æœ€å¾Œæ›´æ–°: 2025-01-13*  
*æ•¸æ“šè¦æ¨¡: 140,782 ç­†*  
*åŸºæ–¼: Supabase å®˜æ–¹æ–‡æª”å’Œæœ€ä½³å¯¦è¸*
